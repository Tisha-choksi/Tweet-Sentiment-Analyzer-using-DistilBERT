{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbcdb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (4.54.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf35fde3-d297-47ed-87c7-10deb8c32220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.28.1\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (2.7.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (4.0.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (0.4.5)\n",
      "Requirement already satisfied: accelerate in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (1.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.28.1) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.28.1) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.28.1) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.28.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.28.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.28.1) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.28.1) (2.32.4)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1)\n",
      "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.28.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: psutil in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers==4.28.1) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers==4.28.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers==4.28.1) (2025.7.14)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers==4.28.1) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\parth\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [62 lines of output]\n",
      "  C:\\Users\\Parth\\AppData\\Local\\Temp\\pip-build-env-nxwhq2kp\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: Apache Software License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.28.1 torch datasets evaluate accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fddb69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da23104",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec61670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweet_data():\n",
    "    print(\"Loading tweet_eval dataset...\")\n",
    "    dataset = load_dataset('tweet_eval', 'sentiment')\n",
    "    return dataset['train'], dataset['validation'], dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48007e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    # Using a model specifically fine-tuned for Twitter sentiment\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac54f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(tokenizer, model):\n",
    "    print(\"Creating sentiment analysis pipeline...\")\n",
    "    sentiment_analyzer = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        top_k=1\n",
    "    )\n",
    "    return sentiment_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea69b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pipeline, test_data, label_names):\n",
    "    print(\"\\nEvaluating model on test data (first 100 samples)...\")\n",
    "    test_samples = test_data.select(range(100))\n",
    "    \n",
    "    true_labels = test_samples['label']\n",
    "    pred_labels = []\n",
    "    \n",
    "    for text in test_samples['text']:\n",
    "        result = pipeline(text)[0][0]\n",
    "        pred_labels.append(int(result['label'].split('_')[-1]))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        true_labels, \n",
    "        pred_labels, \n",
    "        target_names=label_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    print(f\"Accuracy: {accuracy_score(true_labels, pred_labels):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b7d179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweet_eval dataset...\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sentiment analysis pipeline...\n",
      "\n",
      "Evaluating model on test data (first 100 samples)...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.58      0.66      0.61        29\n",
      "     Neutral       0.77      0.67      0.72        55\n",
      "    Positive       0.74      0.88      0.80        16\n",
      "\n",
      "    accuracy                           0.70       100\n",
      "   macro avg       0.69      0.73      0.71       100\n",
      "weighted avg       0.71      0.70      0.70       100\n",
      "\n",
      "Accuracy: 0.70\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Tweet: I love this new feature! It's amazing!\n",
      "Sentiment: Positive (Confidence: 0.99)\n",
      "\n",
      "Tweet: This is the worst experience I've ever had.\n",
      "Sentiment: Negative (Confidence: 0.98)\n",
      "\n",
      "Tweet: The weather is okay today, nothing special.\n",
      "Sentiment: Positive (Confidence: 0.92)\n",
      "\n",
      "Tweet: The service was terrible and the staff was rude!\n",
      "Sentiment: Negative (Confidence: 0.98)\n",
      "\n",
      "Tweet: Just had the best meal of my life at this restaurant!\n",
      "Sentiment: Positive (Confidence: 0.99)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load data and model\n",
    "    train_data, val_data, test_data = load_tweet_data()\n",
    "    tokenizer, model = load_model()\n",
    "    sentiment_pipeline = create_pipeline(tokenizer, model)\n",
    "    \n",
    "    # Define label names\n",
    "    label_names = ['Negative', 'Neutral', 'Positive']\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate_model(sentiment_pipeline, test_data, label_names)\n",
    "    \n",
    "    # Example usage with better formatting\n",
    "    print(\"\\nExample predictions:\")\n",
    "    sample_tweets = [\n",
    "        \"I love this new feature! It's amazing!\",\n",
    "        \"This is the worst experience I've ever had.\",\n",
    "        \"The weather is okay today, nothing special.\",\n",
    "        \"The service was terrible and the staff was rude!\",\n",
    "        \"Just had the best meal of my life at this restaurant!\"\n",
    "    ]\n",
    "    \n",
    "    for tweet in sample_tweets:\n",
    "        result = sentiment_pipeline(tweet)[0][0]\n",
    "        sentiment = label_names[int(result['label'].split('_')[-1])]\n",
    "        print(f\"\\nTweet: {tweet}\")\n",
    "        print(f\"Sentiment: {sentiment} (Confidence: {result['score']:.2f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ceaf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cpu\n",
      "Transformers version: 4.54.1\n",
      "\n",
      "Loading dataset...\n",
      "Loaded 1000 training and 200 validation samples\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Tokenizing data...\n",
      "Tokenization complete!\n",
      "\n",
      "Setting up training...\n",
      "Using v4.x+ parameter names\n",
      "Trainer initialized successfully!\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 24:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.046900</td>\n",
       "      <td>1.016289</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.877400</td>\n",
       "      <td>0.890565</td>\n",
       "      <td>0.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.802343</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.628100</td>\n",
       "      <td>0.787816</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.465600</td>\n",
       "      <td>0.755515</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.414300</td>\n",
       "      <td>0.903981</td>\n",
       "      <td>0.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.927144</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>1.065688</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>1.224716</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>1.322095</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>1.353007</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>1.746580</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>1.557742</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>1.632868</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>1.728593</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>1.722620</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>1.777668</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.795020</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>1.825413</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.810429</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.840344</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.841737</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>1.835852</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.840171</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved.\n",
      "\n",
      "Running test prediction...\n",
      "\n",
      "Text: This product works great!\n",
      "Predicted sentiment: Positive\n",
      "\n",
      "Text: I'm very disappointed with this service\n",
      "Predicted sentiment: Negative\n",
      "\n",
      "Text: The item was okay, nothing special\n",
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "# Verify environment\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# 2. Load dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "try:\n",
    "    dataset = load_dataset('tweet_eval', 'sentiment')\n",
    "    train_data = dataset['train'].select(range(1000))  # Small subset for testing\n",
    "    val_data = dataset['validation'].select(range(200))\n",
    "    print(f\"Loaded {len(train_data)} training and {len(val_data)} validation samples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Model setup\n",
    "print(\"\\nLoading model...\")\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=3,\n",
    "        id2label={0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
    "        label2id={\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# 4. Tokenization\n",
    "print(\"\\nTokenizing data...\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "try:\n",
    "    tokenized_train = train_data.map(tokenize_function, batched=True)\n",
    "    tokenized_val = val_data.map(tokenize_function, batched=True)\n",
    "    print(\"Tokenization complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error tokenizing data: {e}\")\n",
    "    raise\n",
    "\n",
    "# 5. Training configuration with version compatibility\n",
    "print(\"\\nSetting up training...\")\n",
    "try:\n",
    "    # Try newest parameter names first (v4.x+)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./sentiment_results\",\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "        optim=\"adamw_torch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    print(\"Using v4.x+ parameter names\")\n",
    "except TypeError:\n",
    "    # Fallback to older parameter names (pre-v4)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./sentiment_results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\",\n",
    "        optim=\"adamw_torch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    print(\"Using pre-v4 parameter names\")\n",
    "\n",
    "# 6. Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = load(\"accuracy\")\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 7. Initialize Trainer\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print(\"Trainer initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing trainer: {e}\")\n",
    "    raise\n",
    "\n",
    "# 8. Train and save\n",
    "print(\"\\nStarting training...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./final_sentiment_model\")\n",
    "    print(\"Training complete! Model saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    raise\n",
    "\n",
    "# 9. Test prediction\n",
    "print(\"\\nRunning test prediction...\")\n",
    "sample_texts = [\n",
    "    \"This product works great!\",\n",
    "    \"I'm very disappointed with this service\",\n",
    "    \"The item was okay, nothing special\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits).item()\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted sentiment: {model.config.id2label[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "804e943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      "- config.json\n",
      "- model.safetensors\n",
      "- special_tokens_map.json\n",
      "- tokenizer.json\n",
      "- tokenizer_config.json\n",
      "- training_args.bin\n",
      "- vocab.txt\n",
      "\n",
      "Full paths:\n",
      "final_sentiment_model\\config.json\n",
      "final_sentiment_model\\model.safetensors\n",
      "final_sentiment_model\\special_tokens_map.json\n",
      "final_sentiment_model\\tokenizer.json\n",
      "final_sentiment_model\\tokenizer_config.json\n",
      "final_sentiment_model\\training_args.bin\n",
      "final_sentiment_model\\vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# In your Jupyter Notebook - after training is complete\n",
    "from transformers import DistilBertForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# 1. Save the model and tokenizer\n",
    "model.save_pretrained(\"./final_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./final_sentiment_model\")\n",
    "\n",
    "# 2. Verify the files (Windows compatible version)\n",
    "print(\"Saved files:\")\n",
    "for file in os.listdir(\"./final_sentiment_model\"):\n",
    "    print(f\"- {file}\")\n",
    "\n",
    "# Alternative verification (shows full paths)\n",
    "from pathlib import Path\n",
    "model_dir = Path(\"./final_sentiment_model\")\n",
    "print(\"\\nFull paths:\")\n",
    "for file in model_dir.glob('*'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e9596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc593f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
